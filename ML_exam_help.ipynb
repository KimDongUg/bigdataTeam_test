{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.3.2-py3-none-win_amd64.whl (95.2 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\14\\anaconda3\\lib\\site-packages (from xgboost) (1.19.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\14\\anaconda3\\lib\\site-packages (from xgboost) (1.6.0)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-3.1.1-py2.py3-none-win_amd64.whl (754 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\14\\anaconda3\\lib\\site-packages (from lightgbm) (1.19.2)\n",
      "Requirement already satisfied: wheel in c:\\users\\14\\anaconda3\\lib\\site-packages (from lightgbm) (0.35.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\14\\anaconda3\\lib\\site-packages (from lightgbm) (1.6.0)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\14\\anaconda3\\lib\\site-packages (from lightgbm) (0.23.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\14\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\14\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.1.0)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-3.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>custid</th>\n",
       "      <th>gender</th>\n",
       "      <th>총구매액</th>\n",
       "      <th>구매건수</th>\n",
       "      <th>평균구매가격</th>\n",
       "      <th>평균할부개월수</th>\n",
       "      <th>구매브랜드종류</th>\n",
       "      <th>내점일수</th>\n",
       "      <th>수입상품_구매비율</th>\n",
       "      <th>주말방문비율</th>\n",
       "      <th>가을_구매건수</th>\n",
       "      <th>겨울_구매건수</th>\n",
       "      <th>봄_구매건수</th>\n",
       "      <th>여름_구매건수</th>\n",
       "      <th>아침_구매건수</th>\n",
       "      <th>저녁_구매건수</th>\n",
       "      <th>점심_구매건수</th>\n",
       "      <th>주구매코너</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>680100</td>\n",
       "      <td>15</td>\n",
       "      <td>45340</td>\n",
       "      <td>1.7</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>26.7</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>화장품</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>353450</td>\n",
       "      <td>9</td>\n",
       "      <td>39272</td>\n",
       "      <td>1.2</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>11.1</td>\n",
       "      <td>37.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>영캐주얼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>5671400</td>\n",
       "      <td>36</td>\n",
       "      <td>157539</td>\n",
       "      <td>2.8</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>5.6</td>\n",
       "      <td>37.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>장신구</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>1964000</td>\n",
       "      <td>28</td>\n",
       "      <td>70143</td>\n",
       "      <td>1.4</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>39.3</td>\n",
       "      <td>28.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>화장품</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>885000</td>\n",
       "      <td>5</td>\n",
       "      <td>177000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>피혁A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   custid  gender     총구매액  구매건수  평균구매가격  평균할부개월수  구매브랜드종류  내점일수  수입상품_구매비율  \\\n",
       "0      18       0   680100    15   45340      1.7        9    10       26.7   \n",
       "1      21       0   353450     9   39272      1.2        6     8       11.1   \n",
       "2      23       0  5671400    36  157539      2.8       22    16        5.6   \n",
       "3      26       0  1964000    28   70143      1.4       15    14       39.3   \n",
       "4      35       0   885000     5  177000      6.0        5     2        0.0   \n",
       "\n",
       "   주말방문비율  가을_구매건수  겨울_구매건수  봄_구매건수  여름_구매건수  아침_구매건수  저녁_구매건수  점심_구매건수 주구매코너  \n",
       "0   100.0      NaN      9.0     6.0      NaN      NaN      6.0      9.0   화장품  \n",
       "1    37.5      NaN      5.0     4.0      NaN      3.0      NaN      6.0  영캐주얼  \n",
       "2    37.5      7.0     17.0    12.0      NaN     10.0     11.0     15.0   장신구  \n",
       "3    28.6      5.0      8.0    15.0      NaN      NaN     18.0     10.0   화장품  \n",
       "4   100.0      NaN      NaN     5.0      NaN      NaN      5.0      NaN   피혁A  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv', encoding='CP949')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataP = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj=['주구매코너'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataP[obj] = dataP[obj].apply(lambda x: x.astype('category').cat.codes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = ['가을_구매건수','겨울_구매건수','봄_구매건수','여름_구매건수','아침_구매건수','저녁_구매건수','점심_구매건수' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleImputer(strategy='most_frequent')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer_con = SimpleImputer(strategy=\"most_frequent\")  \n",
    "imputer_con.fit(dataP[con])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  9.,  6., ...,  1.,  6.,  9.],\n",
       "       [ 1.,  5.,  4., ...,  3.,  1.,  6.],\n",
       "       [ 7., 17., 12., ..., 10., 11., 15.],\n",
       "       ...,\n",
       "       [12.,  6.,  6., ..., 16.,  1., 21.],\n",
       "       [ 6.,  4.,  2., ...,  1.,  2., 11.],\n",
       "       [ 6.,  5., 10., ...,  8.,  5., 19.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = imputer_con.transform(dataP[con])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataP[con] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX = dataP.drop(['custid','gender'], axis=1) \n",
    "dfy = dataP['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#model = KNeighborsClassifier()\n",
    "model = LogisticRegression()\n",
    "#model = DecisionTreeClassifier()\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.696, 0.697, 0.695, 0.697, 0.695])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(model, dfX, dfy); scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.696\n",
      "Std: 0.001\n",
      "Min: 0.695\n",
      "Max: 0.697\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean: {:.3f}\\nStd: {:.3f}\\nMin: {:.3f}\\nMax: {:.3f}\".format(\n",
    "    scores.mean(), scores.std(), scores.min(), scores.max()))\n",
    "# 분산이 작으면 Overfitting 이 적다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.7.0-py3-none-any.whl (167 kB)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\14\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\14\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.19.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\14\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (0.17.0)\n",
      "Requirement already satisfied: scikit-learn>=0.23 in c:\\users\\14\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (0.23.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\14\\anaconda3\\lib\\site-packages (from scikit-learn>=0.23->imbalanced-learn->imblearn) (2.1.0)\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.7.0 imblearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import *\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.696, 0.697, 0.695, 0.697, 0.695])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(model, dfX, dfy); scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dfX, dfy, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.combine import SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX, yy = SMOTETomek(random_state=0).fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.56      0.64       883\n",
      "           1       0.33      0.52      0.41       367\n",
      "\n",
      "    accuracy                           0.55      1250\n",
      "   macro avg       0.54      0.54      0.52      1250\n",
      "weighted avg       0.62      0.55      0.57      1250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree3 = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
    "tree3.fit(XX, yy)\n",
    "y_pred3 = tree3.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred3))\n",
    "# (precision) 0 이라고 예측한 데이터의 74%만 실제로 0\n",
    "# (precision) 1 이라고 예측한 데이터의 33%만 실제로 1\n",
    "# (recall) 실제 0인 데이터중 56%만 0으로 판별됨 \n",
    "# (recall) 실제 1인 데이타중 52%만 1로 판별됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    883\n",
      "1    367\n",
      "Name: gender, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7064"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "print(y_test.value_counts())\n",
    "DummyClassifier(strategy='most_frequent').fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(data=X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max_depth':3,\n",
    "          'eta':0.1,\n",
    "          'objective':'binary:logistic',\n",
    "          'eval_metric':'logloss',\n",
    "          'early_stoppings':100\n",
    "         }\n",
    "num_rounds = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:05:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { early_stoppings } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:0.67505\teval-logloss:0.67724\n",
      "[1]\ttrain-logloss:0.66029\teval-logloss:0.66433\n",
      "[2]\ttrain-logloss:0.64789\teval-logloss:0.65309\n",
      "[3]\ttrain-logloss:0.63754\teval-logloss:0.64475\n",
      "[4]\ttrain-logloss:0.62892\teval-logloss:0.63700\n",
      "[5]\ttrain-logloss:0.62165\teval-logloss:0.63109\n",
      "[6]\ttrain-logloss:0.61543\teval-logloss:0.62605\n",
      "[7]\ttrain-logloss:0.61015\teval-logloss:0.62206\n",
      "[8]\ttrain-logloss:0.60568\teval-logloss:0.61911\n",
      "[9]\ttrain-logloss:0.60181\teval-logloss:0.61679\n",
      "[10]\ttrain-logloss:0.59833\teval-logloss:0.61454\n",
      "[11]\ttrain-logloss:0.59566\teval-logloss:0.61296\n",
      "[12]\ttrain-logloss:0.59313\teval-logloss:0.61184\n",
      "[13]\ttrain-logloss:0.59081\teval-logloss:0.61034\n",
      "[14]\ttrain-logloss:0.58869\teval-logloss:0.60896\n",
      "[15]\ttrain-logloss:0.58665\teval-logloss:0.60805\n",
      "[16]\ttrain-logloss:0.58462\teval-logloss:0.60762\n",
      "[17]\ttrain-logloss:0.58296\teval-logloss:0.60660\n",
      "[18]\ttrain-logloss:0.58168\teval-logloss:0.60573\n",
      "[19]\ttrain-logloss:0.57988\teval-logloss:0.60511\n",
      "[20]\ttrain-logloss:0.57859\teval-logloss:0.60481\n",
      "[21]\ttrain-logloss:0.57707\teval-logloss:0.60413\n",
      "[22]\ttrain-logloss:0.57599\teval-logloss:0.60361\n",
      "[23]\ttrain-logloss:0.57534\teval-logloss:0.60331\n",
      "[24]\ttrain-logloss:0.57414\teval-logloss:0.60296\n",
      "[25]\ttrain-logloss:0.57339\teval-logloss:0.60271\n",
      "[26]\ttrain-logloss:0.57259\teval-logloss:0.60255\n",
      "[27]\ttrain-logloss:0.57178\teval-logloss:0.60270\n",
      "[28]\ttrain-logloss:0.57081\teval-logloss:0.60249\n",
      "[29]\ttrain-logloss:0.56930\teval-logloss:0.60224\n",
      "[30]\ttrain-logloss:0.56850\teval-logloss:0.60202\n",
      "[31]\ttrain-logloss:0.56786\teval-logloss:0.60209\n",
      "[32]\ttrain-logloss:0.56732\teval-logloss:0.60208\n",
      "[33]\ttrain-logloss:0.56652\teval-logloss:0.60219\n",
      "[34]\ttrain-logloss:0.56591\teval-logloss:0.60211\n",
      "[35]\ttrain-logloss:0.56526\teval-logloss:0.60203\n",
      "[36]\ttrain-logloss:0.56408\teval-logloss:0.60216\n",
      "[37]\ttrain-logloss:0.56320\teval-logloss:0.60199\n",
      "[38]\ttrain-logloss:0.56224\teval-logloss:0.60198\n",
      "[39]\ttrain-logloss:0.56178\teval-logloss:0.60208\n",
      "[40]\ttrain-logloss:0.56083\teval-logloss:0.60187\n",
      "[41]\ttrain-logloss:0.56031\teval-logloss:0.60195\n",
      "[42]\ttrain-logloss:0.55939\teval-logloss:0.60195\n",
      "[43]\ttrain-logloss:0.55851\teval-logloss:0.60184\n",
      "[44]\ttrain-logloss:0.55801\teval-logloss:0.60176\n",
      "[45]\ttrain-logloss:0.55759\teval-logloss:0.60185\n",
      "[46]\ttrain-logloss:0.55723\teval-logloss:0.60191\n",
      "[47]\ttrain-logloss:0.55636\teval-logloss:0.60170\n",
      "[48]\ttrain-logloss:0.55602\teval-logloss:0.60199\n",
      "[49]\ttrain-logloss:0.55554\teval-logloss:0.60199\n",
      "[50]\ttrain-logloss:0.55474\teval-logloss:0.60187\n",
      "[51]\ttrain-logloss:0.55395\teval-logloss:0.60183\n",
      "[52]\ttrain-logloss:0.55369\teval-logloss:0.60191\n",
      "[53]\ttrain-logloss:0.55310\teval-logloss:0.60213\n",
      "[54]\ttrain-logloss:0.55230\teval-logloss:0.60195\n",
      "[55]\ttrain-logloss:0.55152\teval-logloss:0.60212\n",
      "[56]\ttrain-logloss:0.55049\teval-logloss:0.60179\n",
      "[57]\ttrain-logloss:0.55011\teval-logloss:0.60178\n",
      "[58]\ttrain-logloss:0.54944\teval-logloss:0.60156\n",
      "[59]\ttrain-logloss:0.54924\teval-logloss:0.60171\n",
      "[60]\ttrain-logloss:0.54853\teval-logloss:0.60142\n",
      "[61]\ttrain-logloss:0.54828\teval-logloss:0.60145\n",
      "[62]\ttrain-logloss:0.54793\teval-logloss:0.60173\n",
      "[63]\ttrain-logloss:0.54732\teval-logloss:0.60211\n",
      "[64]\ttrain-logloss:0.54670\teval-logloss:0.60219\n",
      "[65]\ttrain-logloss:0.54627\teval-logloss:0.60237\n",
      "[66]\ttrain-logloss:0.54552\teval-logloss:0.60232\n",
      "[67]\ttrain-logloss:0.54519\teval-logloss:0.60226\n",
      "[68]\ttrain-logloss:0.54427\teval-logloss:0.60222\n",
      "[69]\ttrain-logloss:0.54354\teval-logloss:0.60182\n",
      "[70]\ttrain-logloss:0.54287\teval-logloss:0.60199\n",
      "[71]\ttrain-logloss:0.54259\teval-logloss:0.60211\n",
      "[72]\ttrain-logloss:0.54212\teval-logloss:0.60231\n",
      "[73]\ttrain-logloss:0.54152\teval-logloss:0.60244\n",
      "[74]\ttrain-logloss:0.54087\teval-logloss:0.60256\n",
      "[75]\ttrain-logloss:0.54014\teval-logloss:0.60255\n",
      "[76]\ttrain-logloss:0.53979\teval-logloss:0.60264\n",
      "[77]\ttrain-logloss:0.53942\teval-logloss:0.60271\n",
      "[78]\ttrain-logloss:0.53893\teval-logloss:0.60275\n",
      "[79]\ttrain-logloss:0.53858\teval-logloss:0.60319\n",
      "[80]\ttrain-logloss:0.53788\teval-logloss:0.60325\n",
      "[81]\ttrain-logloss:0.53739\teval-logloss:0.60311\n",
      "[82]\ttrain-logloss:0.53722\teval-logloss:0.60318\n",
      "[83]\ttrain-logloss:0.53704\teval-logloss:0.60323\n",
      "[84]\ttrain-logloss:0.53617\teval-logloss:0.60324\n",
      "[85]\ttrain-logloss:0.53533\teval-logloss:0.60364\n",
      "[86]\ttrain-logloss:0.53475\teval-logloss:0.60354\n",
      "[87]\ttrain-logloss:0.53401\teval-logloss:0.60347\n",
      "[88]\ttrain-logloss:0.53355\teval-logloss:0.60368\n",
      "[89]\ttrain-logloss:0.53305\teval-logloss:0.60338\n",
      "[90]\ttrain-logloss:0.53265\teval-logloss:0.60325\n",
      "[91]\ttrain-logloss:0.53213\teval-logloss:0.60315\n",
      "[92]\ttrain-logloss:0.53150\teval-logloss:0.60296\n",
      "[93]\ttrain-logloss:0.53120\teval-logloss:0.60306\n",
      "[94]\ttrain-logloss:0.53063\teval-logloss:0.60326\n",
      "[95]\ttrain-logloss:0.52995\teval-logloss:0.60366\n",
      "[96]\ttrain-logloss:0.52954\teval-logloss:0.60355\n",
      "[97]\ttrain-logloss:0.52879\teval-logloss:0.60366\n",
      "[98]\ttrain-logloss:0.52826\teval-logloss:0.60395\n",
      "[99]\ttrain-logloss:0.52758\teval-logloss:0.60416\n",
      "[100]\ttrain-logloss:0.52736\teval-logloss:0.60434\n",
      "[101]\ttrain-logloss:0.52688\teval-logloss:0.60389\n",
      "[102]\ttrain-logloss:0.52656\teval-logloss:0.60362\n",
      "[103]\ttrain-logloss:0.52591\teval-logloss:0.60406\n",
      "[104]\ttrain-logloss:0.52534\teval-logloss:0.60429\n",
      "[105]\ttrain-logloss:0.52495\teval-logloss:0.60421\n",
      "[106]\ttrain-logloss:0.52436\teval-logloss:0.60432\n",
      "[107]\ttrain-logloss:0.52368\teval-logloss:0.60415\n",
      "[108]\ttrain-logloss:0.52343\teval-logloss:0.60423\n",
      "[109]\ttrain-logloss:0.52329\teval-logloss:0.60430\n",
      "[110]\ttrain-logloss:0.52295\teval-logloss:0.60390\n",
      "[111]\ttrain-logloss:0.52268\teval-logloss:0.60394\n",
      "[112]\ttrain-logloss:0.52207\teval-logloss:0.60433\n",
      "[113]\ttrain-logloss:0.52160\teval-logloss:0.60432\n",
      "[114]\ttrain-logloss:0.52116\teval-logloss:0.60441\n",
      "[115]\ttrain-logloss:0.52074\teval-logloss:0.60456\n",
      "[116]\ttrain-logloss:0.52044\teval-logloss:0.60435\n",
      "[117]\ttrain-logloss:0.52027\teval-logloss:0.60446\n",
      "[118]\ttrain-logloss:0.52003\teval-logloss:0.60465\n",
      "[119]\ttrain-logloss:0.51988\teval-logloss:0.60471\n",
      "[120]\ttrain-logloss:0.51924\teval-logloss:0.60485\n",
      "[121]\ttrain-logloss:0.51840\teval-logloss:0.60491\n",
      "[122]\ttrain-logloss:0.51779\teval-logloss:0.60549\n",
      "[123]\ttrain-logloss:0.51723\teval-logloss:0.60537\n",
      "[124]\ttrain-logloss:0.51647\teval-logloss:0.60562\n",
      "[125]\ttrain-logloss:0.51634\teval-logloss:0.60569\n",
      "[126]\ttrain-logloss:0.51608\teval-logloss:0.60541\n",
      "[127]\ttrain-logloss:0.51564\teval-logloss:0.60530\n",
      "[128]\ttrain-logloss:0.51525\teval-logloss:0.60521\n",
      "[129]\ttrain-logloss:0.51471\teval-logloss:0.60498\n",
      "[130]\ttrain-logloss:0.51414\teval-logloss:0.60514\n",
      "[131]\ttrain-logloss:0.51382\teval-logloss:0.60525\n",
      "[132]\ttrain-logloss:0.51368\teval-logloss:0.60537\n",
      "[133]\ttrain-logloss:0.51344\teval-logloss:0.60527\n",
      "[134]\ttrain-logloss:0.51308\teval-logloss:0.60524\n",
      "[135]\ttrain-logloss:0.51282\teval-logloss:0.60523\n",
      "[136]\ttrain-logloss:0.51235\teval-logloss:0.60509\n",
      "[137]\ttrain-logloss:0.51206\teval-logloss:0.60515\n",
      "[138]\ttrain-logloss:0.51141\teval-logloss:0.60559\n",
      "[139]\ttrain-logloss:0.51078\teval-logloss:0.60549\n",
      "[140]\ttrain-logloss:0.51067\teval-logloss:0.60558\n",
      "[141]\ttrain-logloss:0.51041\teval-logloss:0.60529\n",
      "[142]\ttrain-logloss:0.51001\teval-logloss:0.60541\n",
      "[143]\ttrain-logloss:0.50955\teval-logloss:0.60606\n",
      "[144]\ttrain-logloss:0.50946\teval-logloss:0.60614\n",
      "[145]\ttrain-logloss:0.50934\teval-logloss:0.60606\n",
      "[146]\ttrain-logloss:0.50865\teval-logloss:0.60580\n",
      "[147]\ttrain-logloss:0.50822\teval-logloss:0.60581\n",
      "[148]\ttrain-logloss:0.50780\teval-logloss:0.60597\n",
      "[149]\ttrain-logloss:0.50732\teval-logloss:0.60651\n",
      "[150]\ttrain-logloss:0.50676\teval-logloss:0.60645\n",
      "[151]\ttrain-logloss:0.50667\teval-logloss:0.60654\n",
      "[152]\ttrain-logloss:0.50613\teval-logloss:0.60689\n",
      "[153]\ttrain-logloss:0.50585\teval-logloss:0.60669\n",
      "[154]\ttrain-logloss:0.50533\teval-logloss:0.60662\n",
      "[155]\ttrain-logloss:0.50472\teval-logloss:0.60657\n",
      "[156]\ttrain-logloss:0.50424\teval-logloss:0.60620\n",
      "[157]\ttrain-logloss:0.50361\teval-logloss:0.60616\n",
      "[158]\ttrain-logloss:0.50335\teval-logloss:0.60640\n",
      "[159]\ttrain-logloss:0.50294\teval-logloss:0.60632\n",
      "[160]\ttrain-logloss:0.50243\teval-logloss:0.60623\n",
      "[161]\ttrain-logloss:0.50192\teval-logloss:0.60643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[162]\ttrain-logloss:0.50162\teval-logloss:0.60652\n",
      "[163]\ttrain-logloss:0.50114\teval-logloss:0.60650\n",
      "[164]\ttrain-logloss:0.50081\teval-logloss:0.60659\n",
      "[165]\ttrain-logloss:0.50036\teval-logloss:0.60648\n",
      "[166]\ttrain-logloss:0.50027\teval-logloss:0.60658\n",
      "[167]\ttrain-logloss:0.49972\teval-logloss:0.60645\n",
      "[168]\ttrain-logloss:0.49956\teval-logloss:0.60645\n",
      "[169]\ttrain-logloss:0.49897\teval-logloss:0.60664\n",
      "[170]\ttrain-logloss:0.49866\teval-logloss:0.60647\n",
      "[171]\ttrain-logloss:0.49806\teval-logloss:0.60619\n",
      "[172]\ttrain-logloss:0.49759\teval-logloss:0.60631\n",
      "[173]\ttrain-logloss:0.49721\teval-logloss:0.60650\n",
      "[174]\ttrain-logloss:0.49665\teval-logloss:0.60644\n",
      "[175]\ttrain-logloss:0.49635\teval-logloss:0.60655\n",
      "[176]\ttrain-logloss:0.49601\teval-logloss:0.60646\n",
      "[177]\ttrain-logloss:0.49582\teval-logloss:0.60652\n",
      "[178]\ttrain-logloss:0.49552\teval-logloss:0.60657\n",
      "[179]\ttrain-logloss:0.49506\teval-logloss:0.60660\n",
      "[180]\ttrain-logloss:0.49490\teval-logloss:0.60636\n",
      "[181]\ttrain-logloss:0.49443\teval-logloss:0.60650\n",
      "[182]\ttrain-logloss:0.49399\teval-logloss:0.60652\n",
      "[183]\ttrain-logloss:0.49362\teval-logloss:0.60662\n",
      "[184]\ttrain-logloss:0.49288\teval-logloss:0.60658\n",
      "[185]\ttrain-logloss:0.49238\teval-logloss:0.60633\n",
      "[186]\ttrain-logloss:0.49227\teval-logloss:0.60646\n",
      "[187]\ttrain-logloss:0.49197\teval-logloss:0.60672\n",
      "[188]\ttrain-logloss:0.49167\teval-logloss:0.60674\n",
      "[189]\ttrain-logloss:0.49140\teval-logloss:0.60688\n",
      "[190]\ttrain-logloss:0.49131\teval-logloss:0.60691\n",
      "[191]\ttrain-logloss:0.49104\teval-logloss:0.60720\n",
      "[192]\ttrain-logloss:0.49088\teval-logloss:0.60737\n",
      "[193]\ttrain-logloss:0.49031\teval-logloss:0.60748\n",
      "[194]\ttrain-logloss:0.49017\teval-logloss:0.60726\n",
      "[195]\ttrain-logloss:0.48965\teval-logloss:0.60734\n",
      "[196]\ttrain-logloss:0.48919\teval-logloss:0.60718\n",
      "[197]\ttrain-logloss:0.48909\teval-logloss:0.60731\n",
      "[198]\ttrain-logloss:0.48895\teval-logloss:0.60746\n",
      "[199]\ttrain-logloss:0.48855\teval-logloss:0.60797\n",
      "[200]\ttrain-logloss:0.48801\teval-logloss:0.60783\n",
      "[201]\ttrain-logloss:0.48752\teval-logloss:0.60818\n",
      "[202]\ttrain-logloss:0.48738\teval-logloss:0.60820\n",
      "[203]\ttrain-logloss:0.48724\teval-logloss:0.60836\n",
      "[204]\ttrain-logloss:0.48706\teval-logloss:0.60869\n",
      "[205]\ttrain-logloss:0.48654\teval-logloss:0.60855\n",
      "[206]\ttrain-logloss:0.48620\teval-logloss:0.60899\n",
      "[207]\ttrain-logloss:0.48586\teval-logloss:0.60914\n",
      "[208]\ttrain-logloss:0.48540\teval-logloss:0.60918\n",
      "[209]\ttrain-logloss:0.48506\teval-logloss:0.60906\n",
      "[210]\ttrain-logloss:0.48494\teval-logloss:0.60901\n",
      "[211]\ttrain-logloss:0.48456\teval-logloss:0.60870\n",
      "[212]\ttrain-logloss:0.48448\teval-logloss:0.60862\n",
      "[213]\ttrain-logloss:0.48425\teval-logloss:0.60854\n",
      "[214]\ttrain-logloss:0.48414\teval-logloss:0.60836\n",
      "[215]\ttrain-logloss:0.48362\teval-logloss:0.60857\n",
      "[216]\ttrain-logloss:0.48349\teval-logloss:0.60868\n",
      "[217]\ttrain-logloss:0.48306\teval-logloss:0.60874\n",
      "[218]\ttrain-logloss:0.48257\teval-logloss:0.60865\n",
      "[219]\ttrain-logloss:0.48226\teval-logloss:0.60867\n",
      "[220]\ttrain-logloss:0.48203\teval-logloss:0.60891\n",
      "[221]\ttrain-logloss:0.48151\teval-logloss:0.60895\n",
      "[222]\ttrain-logloss:0.48089\teval-logloss:0.60869\n",
      "[223]\ttrain-logloss:0.48078\teval-logloss:0.60864\n",
      "[224]\ttrain-logloss:0.48057\teval-logloss:0.60888\n",
      "[225]\ttrain-logloss:0.48043\teval-logloss:0.60893\n",
      "[226]\ttrain-logloss:0.47981\teval-logloss:0.60918\n",
      "[227]\ttrain-logloss:0.47965\teval-logloss:0.60913\n",
      "[228]\ttrain-logloss:0.47940\teval-logloss:0.60944\n",
      "[229]\ttrain-logloss:0.47932\teval-logloss:0.60957\n",
      "[230]\ttrain-logloss:0.47874\teval-logloss:0.60955\n",
      "[231]\ttrain-logloss:0.47840\teval-logloss:0.60974\n",
      "[232]\ttrain-logloss:0.47829\teval-logloss:0.60971\n",
      "[233]\ttrain-logloss:0.47799\teval-logloss:0.60954\n",
      "[234]\ttrain-logloss:0.47746\teval-logloss:0.60956\n",
      "[235]\ttrain-logloss:0.47721\teval-logloss:0.60986\n",
      "[236]\ttrain-logloss:0.47703\teval-logloss:0.61000\n",
      "[237]\ttrain-logloss:0.47682\teval-logloss:0.60982\n",
      "[238]\ttrain-logloss:0.47673\teval-logloss:0.60985\n",
      "[239]\ttrain-logloss:0.47653\teval-logloss:0.60993\n",
      "[240]\ttrain-logloss:0.47607\teval-logloss:0.61038\n",
      "[241]\ttrain-logloss:0.47569\teval-logloss:0.61065\n",
      "[242]\ttrain-logloss:0.47523\teval-logloss:0.61050\n",
      "[243]\ttrain-logloss:0.47493\teval-logloss:0.61059\n",
      "[244]\ttrain-logloss:0.47458\teval-logloss:0.61051\n",
      "[245]\ttrain-logloss:0.47407\teval-logloss:0.61103\n",
      "[246]\ttrain-logloss:0.47341\teval-logloss:0.61137\n",
      "[247]\ttrain-logloss:0.47271\teval-logloss:0.61162\n",
      "[248]\ttrain-logloss:0.47219\teval-logloss:0.61175\n",
      "[249]\ttrain-logloss:0.47157\teval-logloss:0.61176\n",
      "[250]\ttrain-logloss:0.47114\teval-logloss:0.61230\n",
      "[251]\ttrain-logloss:0.47049\teval-logloss:0.61195\n",
      "[252]\ttrain-logloss:0.47014\teval-logloss:0.61188\n",
      "[253]\ttrain-logloss:0.46983\teval-logloss:0.61213\n",
      "[254]\ttrain-logloss:0.46968\teval-logloss:0.61238\n",
      "[255]\ttrain-logloss:0.46940\teval-logloss:0.61284\n",
      "[256]\ttrain-logloss:0.46932\teval-logloss:0.61296\n",
      "[257]\ttrain-logloss:0.46878\teval-logloss:0.61307\n",
      "[258]\ttrain-logloss:0.46810\teval-logloss:0.61320\n",
      "[259]\ttrain-logloss:0.46753\teval-logloss:0.61344\n",
      "[260]\ttrain-logloss:0.46703\teval-logloss:0.61356\n",
      "[261]\ttrain-logloss:0.46680\teval-logloss:0.61352\n",
      "[262]\ttrain-logloss:0.46637\teval-logloss:0.61356\n",
      "[263]\ttrain-logloss:0.46620\teval-logloss:0.61367\n",
      "[264]\ttrain-logloss:0.46607\teval-logloss:0.61360\n",
      "[265]\ttrain-logloss:0.46599\teval-logloss:0.61356\n",
      "[266]\ttrain-logloss:0.46558\teval-logloss:0.61374\n",
      "[267]\ttrain-logloss:0.46511\teval-logloss:0.61398\n",
      "[268]\ttrain-logloss:0.46468\teval-logloss:0.61412\n",
      "[269]\ttrain-logloss:0.46456\teval-logloss:0.61422\n",
      "[270]\ttrain-logloss:0.46415\teval-logloss:0.61447\n",
      "[271]\ttrain-logloss:0.46369\teval-logloss:0.61468\n",
      "[272]\ttrain-logloss:0.46322\teval-logloss:0.61456\n",
      "[273]\ttrain-logloss:0.46279\teval-logloss:0.61453\n",
      "[274]\ttrain-logloss:0.46222\teval-logloss:0.61436\n",
      "[275]\ttrain-logloss:0.46198\teval-logloss:0.61437\n",
      "[276]\ttrain-logloss:0.46187\teval-logloss:0.61451\n",
      "[277]\ttrain-logloss:0.46156\teval-logloss:0.61448\n",
      "[278]\ttrain-logloss:0.46151\teval-logloss:0.61441\n",
      "[279]\ttrain-logloss:0.46144\teval-logloss:0.61454\n",
      "[280]\ttrain-logloss:0.46136\teval-logloss:0.61434\n",
      "[281]\ttrain-logloss:0.46103\teval-logloss:0.61425\n",
      "[282]\ttrain-logloss:0.46053\teval-logloss:0.61441\n",
      "[283]\ttrain-logloss:0.46046\teval-logloss:0.61443\n",
      "[284]\ttrain-logloss:0.46019\teval-logloss:0.61437\n",
      "[285]\ttrain-logloss:0.45981\teval-logloss:0.61444\n",
      "[286]\ttrain-logloss:0.45943\teval-logloss:0.61433\n",
      "[287]\ttrain-logloss:0.45931\teval-logloss:0.61441\n",
      "[288]\ttrain-logloss:0.45858\teval-logloss:0.61462\n",
      "[289]\ttrain-logloss:0.45806\teval-logloss:0.61460\n",
      "[290]\ttrain-logloss:0.45783\teval-logloss:0.61474\n",
      "[291]\ttrain-logloss:0.45764\teval-logloss:0.61475\n",
      "[292]\ttrain-logloss:0.45696\teval-logloss:0.61468\n",
      "[293]\ttrain-logloss:0.45634\teval-logloss:0.61478\n",
      "[294]\ttrain-logloss:0.45600\teval-logloss:0.61491\n",
      "[295]\ttrain-logloss:0.45541\teval-logloss:0.61518\n",
      "[296]\ttrain-logloss:0.45497\teval-logloss:0.61523\n",
      "[297]\ttrain-logloss:0.45459\teval-logloss:0.61517\n",
      "[298]\ttrain-logloss:0.45423\teval-logloss:0.61504\n",
      "[299]\ttrain-logloss:0.45408\teval-logloss:0.61474\n",
      "[300]\ttrain-logloss:0.45396\teval-logloss:0.61498\n",
      "[301]\ttrain-logloss:0.45356\teval-logloss:0.61552\n",
      "[302]\ttrain-logloss:0.45307\teval-logloss:0.61542\n",
      "[303]\ttrain-logloss:0.45256\teval-logloss:0.61564\n",
      "[304]\ttrain-logloss:0.45209\teval-logloss:0.61559\n",
      "[305]\ttrain-logloss:0.45143\teval-logloss:0.61575\n",
      "[306]\ttrain-logloss:0.45112\teval-logloss:0.61592\n",
      "[307]\ttrain-logloss:0.45065\teval-logloss:0.61615\n",
      "[308]\ttrain-logloss:0.45041\teval-logloss:0.61610\n",
      "[309]\ttrain-logloss:0.45006\teval-logloss:0.61639\n",
      "[310]\ttrain-logloss:0.44931\teval-logloss:0.61678\n",
      "[311]\ttrain-logloss:0.44895\teval-logloss:0.61649\n",
      "[312]\ttrain-logloss:0.44858\teval-logloss:0.61681\n",
      "[313]\ttrain-logloss:0.44806\teval-logloss:0.61692\n",
      "[314]\ttrain-logloss:0.44756\teval-logloss:0.61696\n",
      "[315]\ttrain-logloss:0.44725\teval-logloss:0.61707\n",
      "[316]\ttrain-logloss:0.44706\teval-logloss:0.61733\n",
      "[317]\ttrain-logloss:0.44654\teval-logloss:0.61715\n",
      "[318]\ttrain-logloss:0.44643\teval-logloss:0.61723\n",
      "[319]\ttrain-logloss:0.44622\teval-logloss:0.61729\n",
      "[320]\ttrain-logloss:0.44616\teval-logloss:0.61733\n",
      "[321]\ttrain-logloss:0.44570\teval-logloss:0.61719\n",
      "[322]\ttrain-logloss:0.44534\teval-logloss:0.61734\n",
      "[323]\ttrain-logloss:0.44509\teval-logloss:0.61738\n",
      "[324]\ttrain-logloss:0.44456\teval-logloss:0.61756\n",
      "[325]\ttrain-logloss:0.44429\teval-logloss:0.61784\n",
      "[326]\ttrain-logloss:0.44396\teval-logloss:0.61810\n",
      "[327]\ttrain-logloss:0.44357\teval-logloss:0.61831\n",
      "[328]\ttrain-logloss:0.44313\teval-logloss:0.61825\n",
      "[329]\ttrain-logloss:0.44280\teval-logloss:0.61812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[330]\ttrain-logloss:0.44251\teval-logloss:0.61819\n",
      "[331]\ttrain-logloss:0.44225\teval-logloss:0.61826\n",
      "[332]\ttrain-logloss:0.44173\teval-logloss:0.61854\n",
      "[333]\ttrain-logloss:0.44140\teval-logloss:0.61839\n",
      "[334]\ttrain-logloss:0.44133\teval-logloss:0.61847\n",
      "[335]\ttrain-logloss:0.44126\teval-logloss:0.61859\n",
      "[336]\ttrain-logloss:0.44118\teval-logloss:0.61855\n",
      "[337]\ttrain-logloss:0.44096\teval-logloss:0.61872\n",
      "[338]\ttrain-logloss:0.44093\teval-logloss:0.61864\n",
      "[339]\ttrain-logloss:0.44088\teval-logloss:0.61876\n",
      "[340]\ttrain-logloss:0.44068\teval-logloss:0.61862\n",
      "[341]\ttrain-logloss:0.44035\teval-logloss:0.61866\n",
      "[342]\ttrain-logloss:0.44003\teval-logloss:0.61894\n",
      "[343]\ttrain-logloss:0.43956\teval-logloss:0.61898\n",
      "[344]\ttrain-logloss:0.43946\teval-logloss:0.61912\n",
      "[345]\ttrain-logloss:0.43942\teval-logloss:0.61923\n",
      "[346]\ttrain-logloss:0.43900\teval-logloss:0.61925\n",
      "[347]\ttrain-logloss:0.43861\teval-logloss:0.61919\n",
      "[348]\ttrain-logloss:0.43838\teval-logloss:0.61951\n",
      "[349]\ttrain-logloss:0.43798\teval-logloss:0.61975\n",
      "[350]\ttrain-logloss:0.43764\teval-logloss:0.62007\n",
      "[351]\ttrain-logloss:0.43730\teval-logloss:0.62043\n",
      "[352]\ttrain-logloss:0.43720\teval-logloss:0.62077\n",
      "[353]\ttrain-logloss:0.43699\teval-logloss:0.62101\n",
      "[354]\ttrain-logloss:0.43674\teval-logloss:0.62078\n",
      "[355]\ttrain-logloss:0.43645\teval-logloss:0.62097\n",
      "[356]\ttrain-logloss:0.43638\teval-logloss:0.62110\n",
      "[357]\ttrain-logloss:0.43572\teval-logloss:0.62121\n",
      "[358]\ttrain-logloss:0.43568\teval-logloss:0.62125\n",
      "[359]\ttrain-logloss:0.43531\teval-logloss:0.62105\n",
      "[360]\ttrain-logloss:0.43508\teval-logloss:0.62089\n",
      "[361]\ttrain-logloss:0.43471\teval-logloss:0.62104\n",
      "[362]\ttrain-logloss:0.43466\teval-logloss:0.62107\n",
      "[363]\ttrain-logloss:0.43447\teval-logloss:0.62112\n",
      "[364]\ttrain-logloss:0.43422\teval-logloss:0.62129\n",
      "[365]\ttrain-logloss:0.43393\teval-logloss:0.62186\n",
      "[366]\ttrain-logloss:0.43359\teval-logloss:0.62195\n",
      "[367]\ttrain-logloss:0.43315\teval-logloss:0.62177\n",
      "[368]\ttrain-logloss:0.43301\teval-logloss:0.62217\n",
      "[369]\ttrain-logloss:0.43262\teval-logloss:0.62230\n",
      "[370]\ttrain-logloss:0.43252\teval-logloss:0.62234\n",
      "[371]\ttrain-logloss:0.43205\teval-logloss:0.62228\n",
      "[372]\ttrain-logloss:0.43154\teval-logloss:0.62236\n",
      "[373]\ttrain-logloss:0.43126\teval-logloss:0.62230\n",
      "[374]\ttrain-logloss:0.43122\teval-logloss:0.62234\n",
      "[375]\ttrain-logloss:0.43078\teval-logloss:0.62261\n",
      "[376]\ttrain-logloss:0.43069\teval-logloss:0.62268\n",
      "[377]\ttrain-logloss:0.43018\teval-logloss:0.62305\n",
      "[378]\ttrain-logloss:0.42975\teval-logloss:0.62356\n",
      "[379]\ttrain-logloss:0.42942\teval-logloss:0.62321\n",
      "[380]\ttrain-logloss:0.42917\teval-logloss:0.62317\n",
      "[381]\ttrain-logloss:0.42900\teval-logloss:0.62341\n",
      "[382]\ttrain-logloss:0.42877\teval-logloss:0.62344\n",
      "[383]\ttrain-logloss:0.42834\teval-logloss:0.62346\n",
      "[384]\ttrain-logloss:0.42801\teval-logloss:0.62366\n",
      "[385]\ttrain-logloss:0.42768\teval-logloss:0.62400\n",
      "[386]\ttrain-logloss:0.42729\teval-logloss:0.62398\n",
      "[387]\ttrain-logloss:0.42686\teval-logloss:0.62417\n",
      "[388]\ttrain-logloss:0.42651\teval-logloss:0.62414\n",
      "[389]\ttrain-logloss:0.42642\teval-logloss:0.62428\n",
      "[390]\ttrain-logloss:0.42599\teval-logloss:0.62441\n",
      "[391]\ttrain-logloss:0.42558\teval-logloss:0.62454\n",
      "[392]\ttrain-logloss:0.42533\teval-logloss:0.62458\n",
      "[393]\ttrain-logloss:0.42506\teval-logloss:0.62468\n",
      "[394]\ttrain-logloss:0.42502\teval-logloss:0.62479\n",
      "[395]\ttrain-logloss:0.42494\teval-logloss:0.62478\n",
      "[396]\ttrain-logloss:0.42443\teval-logloss:0.62471\n",
      "[397]\ttrain-logloss:0.42416\teval-logloss:0.62479\n",
      "[398]\ttrain-logloss:0.42408\teval-logloss:0.62478\n",
      "[399]\ttrain-logloss:0.42373\teval-logloss:0.62476\n"
     ]
    }
   ],
   "source": [
    "# train 데이터 세트는 'train', evaluation(test) 데이터 세트는 'eval'로 명기\n",
    "wlist = [(dtrain, 'train'),(dtest, 'eval')]\n",
    "# 하이퍼 파라미터와 early stoppinig 파라미터를 train() 함수의 파라미터로 전달\n",
    "xgb_model = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_rounds, evals=wlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict() 수행 결과값을 10개만 표시, 예측 확률값으로 표시됨\n",
      "[0.273 0.453 0.275 0.449 0.191 0.158 0.161 0.193 0.18  0.325]\n"
     ]
    }
   ],
   "source": [
    "pred_probs = xgb_model.predict(dtest)\n",
    "print('predict() 수행 결과값을 10개만 표시, 예측 확률값으로 표시됨')\n",
    "print(np.round(pred_probs[:10], 3))\n",
    "\n",
    "preds = [1 if x > 0.5 else 0 for x in pred_probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n",
    "def get_clf_eval(y_test, pred):\n",
    "    confusion = confusion_matrix(y_test, pred)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "    print('오차행렬')\n",
    "    print(confusion)\n",
    "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}'.format(accuracy, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오차행렬\n",
      "[[787  96]\n",
      " [315  52]]\n",
      "정확도: 0.6712, 정밀도: 0.3514, 재현율: 0.1417\n"
     ]
    }
   ],
   "source": [
    "get_clf_eval(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['총구매액', '구매건수', '평균구매가격', '평균할부개월수', '구매브랜드종류', '내점일수', '수입상품_구매비율',\n",
       "       '주말방문비율', '가을_구매건수', '겨울_구매건수', '봄_구매건수', '여름_구매건수', '아침_구매건수',\n",
       "       '저녁_구매건수', '점심_구매건수', '주구매코너'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>총구매액</th>\n",
       "      <th>구매건수</th>\n",
       "      <th>평균구매가격</th>\n",
       "      <th>평균할부개월수</th>\n",
       "      <th>구매브랜드종류</th>\n",
       "      <th>내점일수</th>\n",
       "      <th>수입상품_구매비율</th>\n",
       "      <th>주말방문비율</th>\n",
       "      <th>가을_구매건수</th>\n",
       "      <th>겨울_구매건수</th>\n",
       "      <th>봄_구매건수</th>\n",
       "      <th>여름_구매건수</th>\n",
       "      <th>아침_구매건수</th>\n",
       "      <th>저녁_구매건수</th>\n",
       "      <th>점심_구매건수</th>\n",
       "      <th>주구매코너</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>404285</td>\n",
       "      <td>3</td>\n",
       "      <td>134762</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>33.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2407157</td>\n",
       "      <td>32</td>\n",
       "      <td>75224</td>\n",
       "      <td>1.7</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>9.4</td>\n",
       "      <td>52.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4664283</td>\n",
       "      <td>78</td>\n",
       "      <td>59798</td>\n",
       "      <td>1.8</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>1.3</td>\n",
       "      <td>45.5</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3299200</td>\n",
       "      <td>11</td>\n",
       "      <td>299927</td>\n",
       "      <td>1.9</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>45.5</td>\n",
       "      <td>33.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>555480</td>\n",
       "      <td>5</td>\n",
       "      <td>111096</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      총구매액  구매건수  평균구매가격  평균할부개월수  구매브랜드종류  내점일수  수입상품_구매비율  주말방문비율  가을_구매건수  \\\n",
       "0   404285     3  134762      2.3        3     2       33.3     0.0      1.0   \n",
       "1  2407157    32   75224      1.7       22    17        9.4    52.9      3.0   \n",
       "2  4664283    78   59798      1.8       24    22        1.3    45.5     17.0   \n",
       "3  3299200    11  299927      1.9        9     3       45.5    33.3      1.0   \n",
       "4   555480     5  111096      2.6        4     4       20.0    75.0      2.0   \n",
       "\n",
       "   겨울_구매건수  봄_구매건수  여름_구매건수  아침_구매건수  저녁_구매건수  점심_구매건수  주구매코너  \n",
       "0      1.0     2.0      1.0      1.0      1.0      2.0     22  \n",
       "1      3.0     6.0     20.0      3.0      8.0     21.0     14  \n",
       "2     12.0    29.0     20.0      1.0     23.0     55.0     14  \n",
       "3      1.0    11.0      1.0      1.0      1.0     11.0     16  \n",
       "4      1.0     1.0      3.0      1.0      3.0      2.0     17  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: nestimators\n",
      "[1]\tvalid_0's binary_logloss: 0.684352\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.678241\n",
      "[3]\tvalid_0's binary_logloss: 0.672711\n",
      "[4]\tvalid_0's binary_logloss: 0.666675\n",
      "[5]\tvalid_0's binary_logloss: 0.662707\n",
      "[6]\tvalid_0's binary_logloss: 0.66007\n",
      "[7]\tvalid_0's binary_logloss: 0.657454\n",
      "[8]\tvalid_0's binary_logloss: 0.656766\n",
      "[9]\tvalid_0's binary_logloss: 0.653329\n",
      "[10]\tvalid_0's binary_logloss: 0.651607\n",
      "[11]\tvalid_0's binary_logloss: 0.651391\n",
      "[12]\tvalid_0's binary_logloss: 0.649682\n",
      "[13]\tvalid_0's binary_logloss: 0.649541\n",
      "[14]\tvalid_0's binary_logloss: 0.647985\n",
      "[15]\tvalid_0's binary_logloss: 0.647646\n",
      "[16]\tvalid_0's binary_logloss: 0.64678\n",
      "[17]\tvalid_0's binary_logloss: 0.645509\n",
      "[18]\tvalid_0's binary_logloss: 0.64361\n",
      "[19]\tvalid_0's binary_logloss: 0.643435\n",
      "[20]\tvalid_0's binary_logloss: 0.64084\n",
      "[21]\tvalid_0's binary_logloss: 0.641548\n",
      "[22]\tvalid_0's binary_logloss: 0.64204\n",
      "[23]\tvalid_0's binary_logloss: 0.642742\n",
      "[24]\tvalid_0's binary_logloss: 0.642366\n",
      "[25]\tvalid_0's binary_logloss: 0.641218\n",
      "[26]\tvalid_0's binary_logloss: 0.641837\n",
      "[27]\tvalid_0's binary_logloss: 0.641907\n",
      "[28]\tvalid_0's binary_logloss: 0.641483\n",
      "[29]\tvalid_0's binary_logloss: 0.642078\n",
      "[30]\tvalid_0's binary_logloss: 0.641288\n",
      "[31]\tvalid_0's binary_logloss: 0.641811\n",
      "[32]\tvalid_0's binary_logloss: 0.641291\n",
      "[33]\tvalid_0's binary_logloss: 0.640027\n",
      "[34]\tvalid_0's binary_logloss: 0.641222\n",
      "[35]\tvalid_0's binary_logloss: 0.641227\n",
      "[36]\tvalid_0's binary_logloss: 0.641497\n",
      "[37]\tvalid_0's binary_logloss: 0.641729\n",
      "[38]\tvalid_0's binary_logloss: 0.642938\n",
      "[39]\tvalid_0's binary_logloss: 0.64365\n",
      "[40]\tvalid_0's binary_logloss: 0.644924\n",
      "[41]\tvalid_0's binary_logloss: 0.645082\n",
      "[42]\tvalid_0's binary_logloss: 0.644476\n",
      "[43]\tvalid_0's binary_logloss: 0.64546\n",
      "[44]\tvalid_0's binary_logloss: 0.645877\n",
      "[45]\tvalid_0's binary_logloss: 0.645774\n",
      "[46]\tvalid_0's binary_logloss: 0.646426\n",
      "[47]\tvalid_0's binary_logloss: 0.64719\n",
      "[48]\tvalid_0's binary_logloss: 0.646987\n",
      "[49]\tvalid_0's binary_logloss: 0.647282\n",
      "[50]\tvalid_0's binary_logloss: 0.64835\n",
      "[51]\tvalid_0's binary_logloss: 0.650423\n",
      "[52]\tvalid_0's binary_logloss: 0.650713\n",
      "[53]\tvalid_0's binary_logloss: 0.651047\n",
      "[54]\tvalid_0's binary_logloss: 0.651194\n",
      "[55]\tvalid_0's binary_logloss: 0.650855\n",
      "[56]\tvalid_0's binary_logloss: 0.650562\n",
      "[57]\tvalid_0's binary_logloss: 0.650508\n",
      "[58]\tvalid_0's binary_logloss: 0.651803\n",
      "[59]\tvalid_0's binary_logloss: 0.651212\n",
      "[60]\tvalid_0's binary_logloss: 0.651108\n",
      "[61]\tvalid_0's binary_logloss: 0.651019\n",
      "[62]\tvalid_0's binary_logloss: 0.651602\n",
      "[63]\tvalid_0's binary_logloss: 0.651294\n",
      "[64]\tvalid_0's binary_logloss: 0.651558\n",
      "[65]\tvalid_0's binary_logloss: 0.652591\n",
      "[66]\tvalid_0's binary_logloss: 0.652493\n",
      "[67]\tvalid_0's binary_logloss: 0.651965\n",
      "[68]\tvalid_0's binary_logloss: 0.653527\n",
      "[69]\tvalid_0's binary_logloss: 0.653475\n",
      "[70]\tvalid_0's binary_logloss: 0.654737\n",
      "[71]\tvalid_0's binary_logloss: 0.655479\n",
      "[72]\tvalid_0's binary_logloss: 0.656853\n",
      "[73]\tvalid_0's binary_logloss: 0.657259\n",
      "[74]\tvalid_0's binary_logloss: 0.65726\n",
      "[75]\tvalid_0's binary_logloss: 0.658574\n",
      "[76]\tvalid_0's binary_logloss: 0.659525\n",
      "[77]\tvalid_0's binary_logloss: 0.658892\n",
      "[78]\tvalid_0's binary_logloss: 0.659007\n",
      "[79]\tvalid_0's binary_logloss: 0.659285\n",
      "[80]\tvalid_0's binary_logloss: 0.66023\n",
      "[81]\tvalid_0's binary_logloss: 0.659752\n",
      "[82]\tvalid_0's binary_logloss: 0.660468\n",
      "[83]\tvalid_0's binary_logloss: 0.660965\n",
      "[84]\tvalid_0's binary_logloss: 0.660995\n",
      "[85]\tvalid_0's binary_logloss: 0.661482\n",
      "[86]\tvalid_0's binary_logloss: 0.662024\n",
      "[87]\tvalid_0's binary_logloss: 0.662254\n",
      "[88]\tvalid_0's binary_logloss: 0.662813\n",
      "[89]\tvalid_0's binary_logloss: 0.662532\n",
      "[90]\tvalid_0's binary_logloss: 0.663319\n",
      "[91]\tvalid_0's binary_logloss: 0.663821\n",
      "[92]\tvalid_0's binary_logloss: 0.66488\n",
      "[93]\tvalid_0's binary_logloss: 0.66444\n",
      "[94]\tvalid_0's binary_logloss: 0.663937\n",
      "[95]\tvalid_0's binary_logloss: 0.664651\n",
      "[96]\tvalid_0's binary_logloss: 0.66493\n",
      "[97]\tvalid_0's binary_logloss: 0.66512\n",
      "[98]\tvalid_0's binary_logloss: 0.665409\n",
      "[99]\tvalid_0's binary_logloss: 0.6657\n",
      "[100]\tvalid_0's binary_logloss: 0.666683\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[33]\tvalid_0's binary_logloss: 0.640027\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "lgbm_wrapper = LGBMClassifier(nestimators=400)\n",
    "evals = [(X_test, y_test)]\n",
    "lgbm_wrapper.fit(XX, yy, feature_name=col, early_stopping_rounds=100, \n",
    "                 eval_metric=\"logloss\", eval_set=evals, verbose=True)\n",
    "pred = lgbm_wrapper.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
